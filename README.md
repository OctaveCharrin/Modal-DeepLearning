# Modal-DeepLearning

Deep neural networks have gained popularity in various fields due to their reliability in image classification tasks. However, training these networks typically demands a substantial amount of labeled data. Consequently, significant preprocessing of datasets is required to ensure a properly labeled dataset for supervised learning. An alternative approach to mitigate the costly dataset preparation effort is to label only a small portion of the dataset and train the model using both labeled and unlabeled images, which are often abundant and readily available. This learning paradigm, known as semi-supervised learning, will be investigated as part of the Kaglle competition ”INF473V 2023 Challenge”.

## Report

The theory behind the code is explained in the file `INF473V - REPORT - CharrinOctave - YaiciInes.pdf`.
